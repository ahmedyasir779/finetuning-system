# ğŸ”¥ Fine-tuning System

**Parameter-efficient fine-tuning with LoRA**

## ğŸ¯ Features

- âœ… LoRA (Low-Rank Adaptation) implementation
- âœ… Multiple pre-trained models (ViT, ResNet, ConvNeXt, Swin)
- âœ… Configurable LoRA presets (minimal, balanced, quality, aggressive)
- âœ… Full fine-tuning vs LoRA comparison
- âœ… Hugging Face integration

## ğŸš€ Quick Start
```bash
# Install dependencies
pip install -r requirements.txt

# Run tests
python tests/test_lora_basics.py
```

## ğŸ“Š LoRA Benefits

- **99%+ parameter reduction** compared to full fine-tuning
- **Faster training** (less parameters to update)
- **Lower memory** usage
- **Better generalization** (less overfitting risk)

## ğŸ› ï¸ Tech Stack

- PyTorch 2.1.0
- Transformers 4.36.0
- PEFT 0.7.1
- Hugging Face Hub

---

**Building in Public | Fine-tuning Fundamentals**

Built with â¤ï¸ by Ahmed Yasir
